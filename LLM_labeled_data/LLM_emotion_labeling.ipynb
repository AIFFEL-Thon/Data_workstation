{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdZTSyOCuUgg"
   },
   "source": [
    "# 1. 환경설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YwHUzSrMGCKu",
    "outputId": "949274a6-104a-4cb9-8cf7-6c77009b2875"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.57.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.10.3)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6rE0O0vHYfkn",
    "outputId": "373724eb-abd3-46ce-c86e-76dfe1210318"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: openai\n",
      "Version: 1.57.4\n",
      "Summary: The official Python library for the openai API\n",
      "Home-page: https://github.com/openai/openai-python\n",
      "Author: \n",
      "Author-email: OpenAI <support@openai.com>\n",
      "License: Apache-2.0\n",
      "Location: /usr/local/lib/python3.10/dist-packages\n",
      "Requires: anyio, distro, httpx, jiter, pydantic, sniffio, tqdm, typing-extensions\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pywJ9c4RHtTQ"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iYWyde4eUHCF",
    "outputId": "58bbbbe4-8bbc-4f65-9f28-38b3e57f6b35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API 키 설정\n",
    "openai.api_key = \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3wPtYkMjuQ5y"
   },
   "source": [
    "# 2. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "# 발화 분리, 감정이 태깅된 자유대화(성인) inside\n",
    "file_path = '/content/drive/MyDrive/AIFFELthon/Data/Sample/대화별_발화분리_텍스트+감정_csv_(json에서_변환)/GPT_input/utterance_json_output_inside-GPT_input.csv'  # 업로드된 파일 경로\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 필요한 열만 추출\n",
    "data = data[['FileName', 'Text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PSdu0LzQyqDC"
   },
   "outputs": [],
   "source": [
    "# 파일명 기준으로 대화 분리\n",
    "def group_dialogues_by_filename(data):\n",
    "    # 파일명 기준 그룹화\n",
    "    data['DialogueGroup'] = data['FileName'].str.extract(r'(.+?)_\\d{6}_\\d{6}.wav')\n",
    "    grouped_data = data.groupby('DialogueGroup')['Text'].apply(list).reset_index()\n",
    "    return grouped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7r_DBCHtOWuW"
   },
   "outputs": [],
   "source": [
    "# 대화 데이터 분리\n",
    "grouped_data = group_dialogues_by_filename(data)\n",
    "dialogues = grouped_data['Text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PJjslwh-VigW",
    "outputId": "259520c8-97b8-4378-c1b4-f8281c065817"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['아까 제가 시간 가는 줄도 모르고 계속 얘기를 했네요',\n",
       " '여보세요',\n",
       " '여보세요',\n",
       " '아닙니다',\n",
       " '잘 듣고 있었습니다',\n",
       " '저는',\n",
       " '모두가 그냥',\n",
       " '스키장에서 못 타는',\n",
       " '그럼 보두 그럼',\n",
       " '게임인 줄 알았는데']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogues[1][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. GPT 레이블링\n",
    "- api 호출\n",
    "- 인지적 프롬프팅 : STICC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d1eEHdMqjCXH"
   },
   "outputs": [],
   "source": [
    "def create_chat_completion(sentence, system_input, model=\"gpt-4o-mini\", temperature=1.15, max_tokens=150):\n",
    "    try:\n",
    "        user_input = f\"\"\"\n",
    "        문장: \"{sentence}\"\n",
    "        위 문장을 분석하고 아래 JSON 스키마에 맞춰 결과를 출력해줘:\n",
    "        ```json\n",
    "        {{\n",
    "          \"name\": \"emotion_analysis\",\n",
    "          \"strict\": true,\n",
    "          \"schema\": {{\n",
    "            \"type\": \"object\",\n",
    "            \"required\": [\n",
    "              \"primary_emotion\",\n",
    "              \"secondary_emotion\"\n",
    "            ],\n",
    "            \"properties\": {{\n",
    "              \"primary_emotion\": {{\n",
    "                \"enum\": [\n",
    "                  \"기쁨\",\n",
    "                  \"놀라움\",\n",
    "                  \"두려움\",\n",
    "                  \"사랑스러움\",\n",
    "                  \"슬픔\",\n",
    "                  \"화남\",\n",
    "                  \"없음\"\n",
    "                ],\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"감정 분류 체계에 따라 선택된 가장 가능성 높은 감정.\"\n",
    "              }},\n",
    "              \"secondary_emotion\": {{\n",
    "                \"enum\": [\n",
    "                  \"기쁨\",\n",
    "                  \"놀라움\",\n",
    "                  \"두려움\",\n",
    "                  \"사랑스러움\",\n",
    "                  \"슬픔\",\n",
    "                  \"화남\",\n",
    "                  \"없음\"\n",
    "                ],\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"감정 분류 체계에 따라 선택된 두번째로 가능성 높은 감정.\"\n",
    "              }}\n",
    "            }},\n",
    "            \"additionalProperties\": false\n",
    "          }}\n",
    "        }}\n",
    "        ```\n",
    "        \"\"\"\n",
    "        # 메시지 목록을 자동으로 생성\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_input},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "\n",
    "        response = OpenAI().chat.completions.create(  # return a JSON response\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens  # 최대 토큰 수를 지정\n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "        # return response\n",
    "        clean_content = content.strip(\"```json\").strip(\"```\").strip()\n",
    "        return json.loads(clean_content)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON Decode Error: {e}\")\n",
    "        return {\"error\": \"Invalid JSON format\"}\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# 메시지 목록 예시\n",
    "system_input = \"\"\"\n",
    "**Situation**: 나는 자막에 감정 정보를 반영하는 기술 개발을 위해 text 데이터를 기반으로 감정 레이블링 작업을 하고 있어.\n",
    "\n",
    "**Task**: 네가 문장별로 감정을 분석하고 레이블링을 도와줘. 감정 상태 레이블링 체계는 ‘기쁨,놀라움,두려움,사랑스러움,슬픔,화남,없음’으로 이루어져 있어. 감정 유형 체계는 ‘긍정, 부정, 중립’으로 이루어져 있어. 이 체계를 따라서 레이블링을 해줘. 그리고 감정 상태와 유형 각각 1순위, 2순위를 나누어서 레이블링 해줘.\n",
    "\n",
    "**Intent**: 감정 분류 체계와 감정 유형을 정확히 반영해서 데이터의 품질을 높이는 게 목표야.\n",
    "\n",
    "**Concern**: 텍스트의 문맥이 불분명한 경우 감정을 잘못 분류할까 걱정이야.\n",
    "\n",
    "**Calibration**: 모호한 문장은 '없음'으로 레이블링하되, 가능한 한 분류 체계를 따르도록 노력해줘.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# API 호출 및 결과 출력\n",
    "# responses = create_chat_completion(\"너를 사랑해\",system_input)\n",
    "# print(responses)\n",
    "# print(responses.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rYij1KlfHeAa"
   },
   "outputs": [],
   "source": [
    "sample_dlg = dialogues[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RrQwasI2InqY",
    "outputId": "a0de4acd-c609-4f7a-d30d-00a18b8c65b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['안녕하세요', '네 안녕하세요', '식사는 하셨습니까', '저기 밥을 먹었는지 어쨌는지 지금 모를 정도로', '아 지칩니다', '소리 조금만 켜주시겠어요', '아 제가 얻게 될 힘도 없어요 지금', '아 지금은 쪼끔', '들을 들릴만합니다', '바쁘셨나 봐요', '하는 거 혹시 왜 이렇게 지칠까요', '어 그러게요', '어 우리가 지금 주제가 올림픽인데', '씩씩해야 되는데', '벌써 지치면', '아마 얘기를 하다 보면 더 흥분이 되어서', '아 결혼', '음', '음', '아 목소리 톤이', '음', '올라가지 않을까 싶네요', '어', '휴일인데 많이 바쁘셨나 봐요', '제가 이렇게 바쁜 사람이 아니거든요', '아 그래요', '그런데', '가만히 보면', '네', '무슨 일이 하 어 일어날 때는 한꺼번에 이렇게 겹치는', '아 맞아요', '네', '그렇게 되더라구요 이게 본인 아니게 그렇게 되니까 제가 또 못 미', '빨아주지를 못해요 싹 한 개를 느낄 때가 많네요', '어', '맞아요', '저만 그런 줄 알았는데 다들 그러구나', '또 한가할 땐 너무 한가하고', '아닙니다', '그 동계올림픽', '으이 제가 주제를 가져온 이에 이유가 뭐냐면', '네', '곧 있으면 이제 겨울로 저거 들기 시작하는데', '그렇죠', '그러다 보면', '네 겨울 스포츠가 또 빠질 수는 없겠죠', '네', '으', '그 저희 나라에서', '동계올림픽을', '어 이천 십 팔 년도인가', '개최를 했잖아요', '어', '시작', '그때', '강원도 평창에서 했는데', '네', '그 개회식 때', '그 퍼포 먼스 중에 하나가', '네', '너무 가슴이 벅찰 정도로 감동적인', '그 장면이 있어서', '그걸 한 번 짚고 넘어가고 싶은데 혹시 기억이 나시는나 모르겠는데', '아', '보셨나요', '음 그', '개막식을', '보기는 했는데', '네', '어 그때 작년이 생생하게 떠오르진 않아요', '하지만', '어느 한 장면이 아직도 가슴에 뭉클하게 남는 장면이 있어요', '어떤', '그 우리 올림픽을 상징하는 국기가 오 류 홍기라고 하죠', '네', '그런데 보통 보면 오리온기가 국기로 되어서 이렇게 상징적으로 선수 입장할 때', '맨 먼저 들어오는데', '네', '그렇죠', '저희 나라에서 어', '동계올림픽 개최할 당시', '드론을 띄워서', '오름 뒤에 모양을 만든 거 혹시 기억이 나시나요', '저는 전혀 보질 못해서', '아 그런 게 있었어요', '아 그러셨구나', '나중에라도', '멋있었겠다', '저하고 대화가 끝나면', '네', '한 번 꼭 검색해서 보세요', '너무너무 가슴 우쿨한 장면이었는데', '어 이거는 뭐 저 혼자 말은 생각인가는 모르겠는데', '처음이랑 그게 시즌인 줄 알았어요 저는', '음', '그런데', '그게 드론을 그때 당시에 하늘로 띄웠던 게', '오리온기의 모양을 만들기 위해서 어 몇 대를 뛰었다 하더라 굉장히 많은', '드로를 띄웠는데', '그렇게', '그 오렌지의 모양을 만들려면', '네', '기술이 어마어마해야 되겠죠', '상상만 해도 어려울 거 같은데', '그 모양을 만든 채로 계속', '그렇죠', '음', '그때 당시에', '그게 하나만 시스템이 잘못돼도', '오륜기  형태가', '이빠진 것처럼', '그렇죠', '굉장히 보기 싫었을 건데', '음', '어', '쓸 수 없이', '그때 완벽하게 퍼포먼스를 펼쳤던 게 있거든요 꼭 한 번 찾아보세요 그래서 저희 나라의 기술력이 이렇게까지 발전을 했고 대단하구나 하면서 자부심을', '네', '음', '제가 한 거는 아니지만 굉장한 자부심을 느꼈었어요 지금 얘기하면서도 또 흥분이 되다 보니까', '네', '음', '목소리 톤이 올라가네요', '네', '아니 저는 보질 못 했는데', '저는 그 올림픽 평창 올림픽을 할 때 제가 다른 기억이 있어서', '보지는 못했지만', '그 지금 설명을 들어보니까', '어 그게 쉽지 않을 텐데 그걸', '쓸 수 없이 그 완성했다면', '전 세계적으로 굉장한 이슈가 됐었겠어요', '어 그때 전 세계가 깜짝 놀랐죠', '꼭 봐야겠네요', '뭐 두 돈을 뛰었을 거란 생각을', '제가 나중에 받는지 안 받는지 확인할 거예요', '어 원래 아니 이건', '제가 궁금해서라도 봐야 될 거 같은데요', '정말로 얼 얻을만한 멋있게 잘 뛰었을까', '그럼 한 두 사람이 드론을 조정한 게 아니었을 텐데', '그 사람들 하나하나가 그걸 맞췄다는 거잖아요', '그게', '그만큼에 사람의 수가 들어가는 게 아니라', '네', '그', '한 사람 있어 그거를 컴퓨터예', '네', '입력을 해서', '한 사람이 그거를 다 제가 만든 걸로 알고 있거든요', '아', '한 사람인지 몇 사람인지는 모르는데', '음', '저도 그 드론이라는 게 각자가 띄워서 그렇게 형태를 만든 줄 알았어요', '네', '네', '그런데', '그', '드론 수 숫자만큼 사람 수가 필요한 게 아니라', '네', '어 사람 숫자는 몇 명 안 되지만', '컴퓨터에서 조작을 해서', '굉장히 섬세한', '이용해서', '그렇게 드론에', '뛰었다 하더라구요', '어', '놀랐네요', '제가 설명이 좀 부족한데 나중에 찾아보실 때', '저희 나라', '아마 그 설명도 한번 들어보세요', '정말 어마어마한', '기술이에요', '아니 저희 나라 과학기술이', '그렇게 많이 발전했나요', '그렇죠 그', '상상이 안 가는데', '올림픽 하면 항상 그', '시대에 따라서 퍼포먼스가 생각지도 못하는 게 나오잖아요', '그러죠', '그런데 동계올림픽에서', '드론으로 오렌지를 만든 거는', '굉장한', '성과라고 봐야 되겠지요', '음', '아니 꼭 찾아보겠다고', '이 약속을 드리기 전에 제가 궁금해서 제가 먼저 찾아볼 것 같네요', '정말 궁금하네요 어땠을', '혹시 뭐 동계올림픽을 생각하면 아 그 이전에', '음', '겨울에 하는 스포츠를 뭐 즐겨 하신다거나 해보신 게 있나요', '겨울에 하는 스포츠는', '이제 예전에', '놀이동산이라고 하죠', '놀이동산에 가면 그', '아이스', '스케이팅', '스케이팅을', '어렸을', '때 왜 이렇게', '보잖아요 거기', '어 이런', '어', '장소를 얘기해도 될지 모르겠지만 롯데월드에 가면', '아 말씀하세요', '네', '롯데월드에 가면', '그 스케이트장이 있거든요', '실내에', '그 저는 그걸 어릴 때', '보고 굉장히 어릴 때 봤어요', '어릴 때 보고 스케이트를 너무 타보고 싶더라구요', '근데', '그때 볼당시에는', '너무 어려서 그걸 타볼 엄두를 못 냈고', '어 초등학교 들어가던가', '시골에 방학에 겨울방학에', '시골에 놀러를 가면', '사천 오빠들이', '그 그때는 그런 게 많았어요 이렇게 논이나 강을 얼려서', '입장료를 받고', '스케이트를 타개해 주는', '그런 곳이 많았어요', '거기서', '잠깐 다 봤던 거 같애요', '스케이트를', '근데', '얼마 못하고', '자꾸 넘어지고 넘어지고 하다 보니까 겁이 나서', '못 탔던 기억이 있거든요', '응 정말 배워보고 싶었던 것 중에 하나가 스케이트였고', '그다음에', '성인이 되고 나서', '배워보고 싶었던 게', '보드라고 하세요 보드', '아 네 한때는 저희 아이들도', '네', '그 스키장에 가서 저는 일반 스키만을 생각을 했는데', '네', '으 늘', '생긴 판자같이 생긴', '네', '어 그런 거를 타고 내려가는 거라고 하더라고요', '혹시 어렸을 때 그 일 명 비료 푸드라고 하죠', '그런 거 타보신 적 없으세요', '아이고 제가 그 세 대 이런 사람이지요', '제가 그 일 명', '비료 오픈해 자루', '출신이에요', '아 그래요', '그거를 앉아서 타셨나요 서서 타셨나요', '아이고 앉아서 탔죠 서서 탄다는 거는 꿈에도 생각할 수 없었죠', '아 그래요', '저도 거의', '앉아서 타다시피 했는데 왜 가끔씩 이렇게 서서', '그 자루 앞을 잡고', '이렇게 서', '서다시피 해서 타본 적이 있어요', '보드가 그거랑 비슷하거든요', '그냥 일명 뒤로 푸대해 발을 올리구', '그냥 서서 타는 그런 건데', '인제', '아 저 제가 그걸 타봤다는 게 아니라 그걸 배워볼려고', '이제 한참 보드가 유행하기 시작할 때쯤에', '되어 보려고 갔는데', '엄두가 안 나더라구요', '아주 그러니까', '어렸을 때', '비로 푸드를 생각하고', '쉽게 탈 수 있을까라 생각했는데', '이 그때에', '그 어릴 때 비록 부대랑은 다르게 좀 면적도 짧고', '또 발을 끼우는 부분이 있더라구요', '거기에 고정을 하고 타야 되니까', '이 몸놀림이 허리힘이 상당히 좋아야 된다는 생각', '요 하체 힘도 좋아야 되고', '그래서 그걸 포기하고 배운 게 스키였거든요', '네 스키는 탈 줄 알아요', '아', '네 스키는 조금', '보드보다 근데', '어 저는 어렵게 생각했는데', '나중에 스키를 타고 나서 보니까', '스키보다 보드가 배우기가 쉽더라구요', '배우기가 쉽고 타기도 쉽고', '아 그럼 스키를 타고 나신 후에 도전해보실 생각은 안 하셨어요', '네', '아', '게 이미 스키에 몸이 익혀져 있으니까 이게', '보드랑 스키랑 차이점이 뭐냐면', '스키는 체중을', '어 앞으로', '쏟아야 돼요', '뒤로 쏟으면', '넘어져요 일 명 자빠진다고 하죠', '그렇게', '앞으로 쏟아야 되는데', '보드는', '뒤로 쏟아야 돼요', '그러니까 이게', '스키에 이 몸을 맞춰 노니까', '모두가 안 되더라구요', '그래서 뭐 가서 도전을 해볼 수도 있었는데', '아 그게 탈 때 그', '네 탈 때에 스키와 보드를 탈 때', '무게 중심이 다르네요', '네', '그렇죠', '으 그러니까', '보드는', '그 우리 어렸을 때 비려 푸듯한다고 생각하시면', '딱 좋은 게', '비로소 탈 때 몸을 앞으로 이렇게 숙이면 꼬끄라지잖아요', '그것처럼 이렇게 거의 누군 자세로 이렇게 탔던 기억이 있는데', '기억나세요', '거의', '어', '네 거의 그런 식으로', '이제', '무게 중심이 뒤로 가 있는 거죠', '근데', '나중에', '스키랑 보드를 다 타는 애들이 그러는데 보드가 배우기도 쉽고', '더 스릴 있다고 하는데', '짧잖아요 겨울이', '그리고 스키 타는 것도 시간도 짧은데', '그 시간에', '스키를 더 즐기고 말지 언제 또 보드를 배워 이런 생각에', '아마 안 배웠던 거 같애요', '정말 시간이 되면 한번 배워보고 싶은데', '보통 보드 타는 애들 보면', '왠지 멋있어 보이구', '여자들도', '남자들도 그렇고', '어 굉장히', '뭐라 그래야 되나', '날렵해 보인다고 해야 되나', '그렇더라고요', '네 스키는 이제 쫌', '아 그룹', '응', '까 몸에', '이', '어', '아 제가 지금 움직이면서 해보니까', '모음을 다 써야 되는 게 보드구요', '스키 같은 경우는', '두 다리에만', '그 힘을 딱 주면 충분히 탈 수 있는 건데', '일단', '배우기는 보드가 더 쉽다는 얘기를 들은 적이 있어요', '근데', '아 그렇구나', '동계올림픽 종목에 보조는 없고 스키는 있죠', '아니요 이게 보드가', '네', '어 정식 종목으로 채택이 되어서', '아 그래요', '어떤 게 올림픽에도', '굉장히 볼거리 중 하나에 종목으로 제가 알고 있어요', '아', '그리 우리 쉽게 생각하면', '아', '보드는', '위에서만 타고 내려오는 걸로만 생각하잖아요', '네', '그런데 그 종목이 굉장히 여러 가지의 종목으로 또 나뉘어져 있더라고요', '나눌 게 뭐가 있죠', '쉽게 말하면', '더 높은 곳에서', '그 저 밑에 있는 초입 어 도착지점까지', '그냥 보드를 타고', '내려오는 그 단순한 게임이 있는가 하면', '음', '또 보드를 타고', '뭐라고 표현을 할까요', '둥근 원통을 위에 싹둑 잘라버리면', '어떻게 되겠어요', '이렇게 길이 깊이 파진 것처럼 그런 길이 나있을 거 아니에요', '그렇죠', '그 얼음', '귀를', '지그재그로 하면서 그 원심력을 #@이름#서 최대한 높이 올라가서', '거기서 저 또 어 요구해지는 기술이 있더라구요', '아', '그러면서', '날아 올라서 빙글빙글 도는 게 있고', '음', '또 몸을 틀어서 착취하는 게 있고', '그런데 눈 위에서 하다 보니까 많이 미끄러지잖아요', '네', '어 그 종목 이름을 제가 잘 모르겠는데 아무튼 보드를 타고 하는 그 종목이 그런 게 또 있고', '응', '또 위에서', '내려오면서', '어 장애물을', '설치해 놓고', '내려올 때마다 그 장애물을 다 통과를 해서', '어 도착 지점까지', '가야 되는 그런 게임도 있고', '보드를 타고 하는 게', '아마 몇 종류 몇 종류 되는 걸로 알아요', '아', '그렇구나', '왜', '겨울에 스키장 가면', '뭐 스키 같은 경우는', '크게 기술이랄 게 없는데', '보드 타는 애들이', '이렇게 점프를 한다고 해야 되나', '그렇게 잠깐 잠깐의 기술을 부리는 애들 보긴 봤거든요', '그게 그 기술의 하나였나', '멋있던데', '이렇게 한 바퀴 돌거나', '그러니까 이게', '네', '그런 기술들', '이게 뭐라 그래야 될까', '단순하게 이제 내려오는 거는', '어', '싱겁다라고 해야 되나', '어', '그래서', '아마 이제', '그런 것도', '도 조금', '발전을 시켜서', '이제 하다 보니까', '여기 기본적인 게 있고', '떡 거기에다가', '기술 점수가 들어가는 게 있고', '또 거기다가 예술점수도 포함되는', '그런 총목도 있고', '음', '하나에 장비 보드를 가지고 하는데', '본 게임 자체는', '쪼금 더 세분화가 되어서 나뉘어져 있더라구요', '으', '근데 제가', '스키장을 가봐서 아는데', '보도에 그런 기술을 넣고 한다면 물론 숙달이 돼야 되겠지만', '그래도 왜', '원숭이도 나물에서 떨어진다구', '어', '잘 안될 때가 있을까 하네요', '만약에 그게 기술을 요하다가', '뭐', '방향을 잘못 틀었거나 해서', '떨어진다거나 이러면', '상당히 아프거든요 스키를 타다가', '눈 위에 너무 안 져도', '그 부상이 상당한데', '음 거의 좀', '위험하겠어요 그 스포츠 자체가', '위험하죠 그래서 눈 위에서 하는', '굉장히', '어 그런 운동들은 보면', '보호 장구 장비가', '굉장히 필수적이죠', '음', '네 선수들이', '어 게임을 할 때 보면', '헬맷은 필수적으로 써야 되고', '무릎 보호대', '팔꿈치 보호대', '음', '그런데 장비들을 착용하고', '대회에 나오거든요', '어', '어 이렇게 하다 보면 이제', '기술을 요 핫', '종목은', '선수들이 하다 보면', '집중력이 흐트러지거나', '도움 답기가 모잘라서', '한 올로 뛰어올랐는데', '착취할 때', '엉덩이 방아를 찢는 선수도 있었고', '어 내려오다가 눈 위를 몇 바퀴 구르면서 떨어진 선수도 있었고', '솔직이  좀 겨울']]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(sample_dlg[:5])  # 첫 5개의 문장 확인\n",
    "print(type(sample_dlg[0]))  # 첫 번째 문장의 데이터 타입 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 저장 리스트\n",
    "results = []\n",
    "\n",
    "for i, sentence in enumerate(sample_dlg[0]):\n",
    "    print(f\"Processing sentence {i + 1}/{len(sample_dlg[0])}\")\n",
    "    try:\n",
    "        # 감정 레이블링 수행\n",
    "        result = create_chat_completion(sentence, system_input)\n",
    "        results.append({\n",
    "            \"sentence_id\": i + 1,\n",
    "            \"sentence\": sentence,\n",
    "            \"result\": result  # 감정 레이블링 결과\n",
    "        })\n",
    "    except json.JSONDecodeError as e:\n",
    "        # JSON 디코드 오류 처리\n",
    "        print(f\"JSON Decode Error: {e} for sentence: {sentence}\")\n",
    "        results.append({\n",
    "            \"sentence_id\": i + 1,\n",
    "            \"sentence\": sentence,\n",
    "            \"result\": {\"error\": f\"JSON Decode Error: {e}\"}\n",
    "        })\n",
    "    except Exception as e:\n",
    "        # 일반 오류 처리\n",
    "        print(f\"Error: {e} for sentence: {sentence}\")\n",
    "        results.append({\n",
    "            \"sentence_id\": i + 1,\n",
    "            \"sentence\": sentence,\n",
    "            \"result\": {\"error\": str(e)}\n",
    "        })\n",
    "\n",
    "    # 현재 처리된 결과를 주기적으로 저장 (안전한 데이터 보존)\n",
    "    if (i + 1) % 50 == 0:  # 50문장마다 중간 저장\n",
    "        temp_output_file = f\"temp_emotion_analysis_{i + 1}.json\"\n",
    "        with open(temp_output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Temporary results saved to {temp_output_file}\")\n",
    "\n",
    "# 최종 결과를 JSON 파일로 저장\n",
    "output_file = \"emotion_analysis_sentences.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M55IbVl5yfGU"
   },
   "source": [
    "### 기존 저장된 것 이후부터 진행하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ISYcASe2MZQ0",
    "outputId": "6bf25fd8-b9ef-47e7-841d-b2c2e43e2aee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 300 sentences from previous results.\n",
      "Processing sentence 301/478\n",
      "Processing sentence 302/478\n",
      "Processing sentence 303/478\n",
      "Processing sentence 304/478\n",
      "Processing sentence 305/478\n",
      "Processing sentence 306/478\n",
      "Processing sentence 307/478\n",
      "Processing sentence 308/478\n",
      "Processing sentence 309/478\n",
      "Processing sentence 310/478\n",
      "Processing sentence 311/478\n",
      "Processing sentence 312/478\n",
      "Processing sentence 313/478\n",
      "JSON Decode Error: Extra data: line 5 column 1 (char 59)\n",
      "Processing sentence 314/478\n",
      "Processing sentence 315/478\n",
      "Processing sentence 316/478\n",
      "Processing sentence 317/478\n",
      "Processing sentence 318/478\n",
      "Processing sentence 319/478\n",
      "Processing sentence 320/478\n",
      "Processing sentence 321/478\n",
      "Processing sentence 322/478\n",
      "Processing sentence 323/478\n",
      "Processing sentence 324/478\n",
      "Processing sentence 325/478\n",
      "Processing sentence 326/478\n",
      "Processing sentence 327/478\n",
      "Processing sentence 328/478\n",
      "Processing sentence 329/478\n",
      "Processing sentence 330/478\n",
      "Processing sentence 331/478\n",
      "Processing sentence 332/478\n",
      "Processing sentence 333/478\n",
      "Processing sentence 334/478\n",
      "Processing sentence 335/478\n",
      "Processing sentence 336/478\n",
      "Processing sentence 337/478\n",
      "Processing sentence 338/478\n",
      "Processing sentence 339/478\n",
      "Processing sentence 340/478\n",
      "Processing sentence 341/478\n",
      "Processing sentence 342/478\n",
      "Processing sentence 343/478\n",
      "Processing sentence 344/478\n",
      "Processing sentence 345/478\n",
      "Processing sentence 346/478\n",
      "Processing sentence 347/478\n",
      "Processing sentence 348/478\n",
      "Processing sentence 349/478\n",
      "Processing sentence 350/478\n",
      "Temporary results saved to temp_emotion_analysis_350.json\n",
      "Processing sentence 351/478\n",
      "Processing sentence 352/478\n",
      "Processing sentence 353/478\n",
      "Processing sentence 354/478\n",
      "Processing sentence 355/478\n",
      "Processing sentence 356/478\n",
      "Processing sentence 357/478\n",
      "Processing sentence 358/478\n",
      "Processing sentence 359/478\n",
      "Processing sentence 360/478\n",
      "Processing sentence 361/478\n",
      "Processing sentence 362/478\n",
      "Processing sentence 363/478\n",
      "Processing sentence 364/478\n",
      "Processing sentence 365/478\n",
      "Processing sentence 366/478\n",
      "Processing sentence 367/478\n",
      "Processing sentence 368/478\n",
      "Processing sentence 369/478\n",
      "Processing sentence 370/478\n",
      "Processing sentence 371/478\n",
      "JSON Decode Error: Extra data: line 5 column 1 (char 59)\n",
      "Processing sentence 372/478\n",
      "Processing sentence 373/478\n",
      "Processing sentence 374/478\n",
      "Processing sentence 375/478\n",
      "Processing sentence 376/478\n",
      "Processing sentence 377/478\n",
      "Processing sentence 378/478\n",
      "Processing sentence 379/478\n",
      "Processing sentence 380/478\n",
      "Processing sentence 381/478\n",
      "Processing sentence 382/478\n",
      "Processing sentence 383/478\n",
      "Processing sentence 384/478\n",
      "Processing sentence 385/478\n",
      "Processing sentence 386/478\n",
      "Processing sentence 387/478\n",
      "JSON Decode Error: Extra data: line 5 column 1 (char 59)\n",
      "Processing sentence 388/478\n",
      "Processing sentence 389/478\n",
      "Processing sentence 390/478\n",
      "Processing sentence 391/478\n",
      "Processing sentence 392/478\n",
      "Processing sentence 393/478\n",
      "Processing sentence 394/478\n",
      "Processing sentence 395/478\n",
      "Processing sentence 396/478\n",
      "Processing sentence 397/478\n",
      "Processing sentence 398/478\n",
      "Processing sentence 399/478\n",
      "Processing sentence 400/478\n",
      "Temporary results saved to temp_emotion_analysis_400.json\n",
      "Processing sentence 401/478\n",
      "Processing sentence 402/478\n",
      "Processing sentence 403/478\n",
      "Processing sentence 404/478\n",
      "Processing sentence 405/478\n",
      "Processing sentence 406/478\n",
      "Processing sentence 407/478\n",
      "Processing sentence 408/478\n",
      "Processing sentence 409/478\n",
      "Processing sentence 410/478\n",
      "Processing sentence 411/478\n",
      "Processing sentence 412/478\n",
      "Processing sentence 413/478\n",
      "Processing sentence 414/478\n",
      "Processing sentence 415/478\n",
      "Processing sentence 416/478\n",
      "Processing sentence 417/478\n",
      "Processing sentence 418/478\n",
      "Processing sentence 419/478\n",
      "Processing sentence 420/478\n",
      "Processing sentence 421/478\n",
      "Processing sentence 422/478\n",
      "Processing sentence 423/478\n",
      "Processing sentence 424/478\n",
      "Processing sentence 425/478\n",
      "Processing sentence 426/478\n",
      "Processing sentence 427/478\n",
      "Processing sentence 428/478\n",
      "Processing sentence 429/478\n",
      "Processing sentence 430/478\n",
      "Processing sentence 431/478\n",
      "Processing sentence 432/478\n",
      "Processing sentence 433/478\n",
      "Processing sentence 434/478\n",
      "Processing sentence 435/478\n",
      "Processing sentence 436/478\n",
      "Processing sentence 437/478\n",
      "Processing sentence 438/478\n",
      "Processing sentence 439/478\n",
      "Processing sentence 440/478\n",
      "Processing sentence 441/478\n",
      "Processing sentence 442/478\n",
      "Processing sentence 443/478\n",
      "Processing sentence 444/478\n",
      "Processing sentence 445/478\n",
      "Processing sentence 446/478\n",
      "Processing sentence 447/478\n",
      "Processing sentence 448/478\n",
      "Processing sentence 449/478\n",
      "Processing sentence 450/478\n",
      "Temporary results saved to temp_emotion_analysis_450.json\n",
      "Processing sentence 451/478\n",
      "Processing sentence 452/478\n",
      "Processing sentence 453/478\n",
      "Processing sentence 454/478\n",
      "Processing sentence 455/478\n",
      "Processing sentence 456/478\n",
      "Processing sentence 457/478\n",
      "JSON Decode Error: Extra data: line 5 column 1 (char 59)\n",
      "Processing sentence 458/478\n",
      "Processing sentence 459/478\n",
      "Processing sentence 460/478\n",
      "Processing sentence 461/478\n",
      "Processing sentence 462/478\n",
      "Processing sentence 463/478\n",
      "Processing sentence 464/478\n",
      "Processing sentence 465/478\n",
      "Processing sentence 466/478\n",
      "Processing sentence 467/478\n",
      "Processing sentence 468/478\n",
      "Processing sentence 469/478\n",
      "Processing sentence 470/478\n",
      "Processing sentence 471/478\n",
      "Processing sentence 472/478\n",
      "Processing sentence 473/478\n",
      "Processing sentence 474/478\n",
      "Processing sentence 475/478\n",
      "Processing sentence 476/478\n",
      "Processing sentence 477/478\n",
      "Processing sentence 478/478\n",
      "Results saved to /content/temp_emotion_analysis_300.json\n"
     ]
    }
   ],
   "source": [
    "# 기존에 저장된 결과 파일 읽기\n",
    "results = []\n",
    "processed_ids = set()  # 처리된 문장의 ID를 저장\n",
    "\n",
    "output_file = \"/content/temp_emotion_analysis_300.json\"\n",
    "\n",
    "if os.path.exists(output_file):\n",
    "    with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        results = json.load(f)\n",
    "        processed_ids = {item[\"sentence_id\"] for item in results}  # 이미 처리된 문장의 ID\n",
    "\n",
    "print(f\"Loaded {len(processed_ids)} sentences from previous results.\")\n",
    "\n",
    "# 301번부터 시작\n",
    "for i, sentence in enumerate(sample_dlg[0]):\n",
    "    sentence_id = i + 1\n",
    "    if sentence_id in processed_ids:\n",
    "        continue  # 이미 처리된 문장은 건너뜀\n",
    "\n",
    "    print(f\"Processing sentence {sentence_id}/{len(sample_dlg[0])}\")\n",
    "    try:\n",
    "        # 감정 레이블링 수행\n",
    "        result = create_chat_completion(sentence, system_input)\n",
    "        results.append({\n",
    "            \"sentence_id\": sentence_id,\n",
    "            \"sentence\": sentence,\n",
    "            \"result\": result  # 감정 레이블링 결과\n",
    "        })\n",
    "    except json.JSONDecodeError as e:\n",
    "        # JSON 디코드 오류 처리\n",
    "        print(f\"JSON Decode Error: {e} for sentence: {sentence}\")\n",
    "        results.append({\n",
    "            \"sentence_id\": sentence_id,\n",
    "            \"sentence\": sentence,\n",
    "            \"result\": {\"error\": f\"JSON Decode Error: {e}\"}\n",
    "        })\n",
    "    except Exception as e:\n",
    "        # 일반 오류 처리\n",
    "        print(f\"Error: {e} for sentence: {sentence}\")\n",
    "        results.append({\n",
    "            \"sentence_id\": sentence_id,\n",
    "            \"sentence\": sentence,\n",
    "            \"result\": {\"error\": str(e)}\n",
    "        })\n",
    "\n",
    "    # 현재 처리된 결과를 주기적으로 저장 (안전한 데이터 보존)\n",
    "    if sentence_id % 50 == 0:  # 50문장마다 중간 저장\n",
    "        temp_output_file = f\"temp_emotion_analysis_{sentence_id}.json\"\n",
    "        with open(temp_output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Temporary results saved to {temp_output_file}\")\n",
    "\n",
    "# 최종 결과를 JSON 파일로 저장\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
